import random  # Import the random module
from dataclasses import asdict

import streamlit as st
import torch
from modelscope import snapshot_download
from transformers import AutoTokenizer, AutoModelForCausalLM  # isort: skip
from transformers.utils import logging

from interface import GenerationConfig, generate_interactive

MODEL_DIR = snapshot_download("YueZhengMeng/InternLM2_Hod_7B", cache_dir="./InternLM2_Hod_7B")
logger = logging.get_logger(__name__)
user_prompt = '<|im_start|>user\n{user}<|im_end|>\n'
robot_prompt = '<|im_start|>assistant\n{robot}<|im_end|>\n'
cur_query_prompt = '<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n'


def on_btn_click():
    del st.session_state.messages


@st.cache_resource
def load_model():
    # int4 é‡åŒ–åŠ è½½
    # quantization_config = BitsAndBytesConfig(
    #     load_in_4bit=True,
    #     bnb_4bit_compute_dtype=torch.float16,
    #     bnb_4bit_quant_type="nf4",
    #     bnb_4bit_use_double_quant=True,
    # )
    print("æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, trust_remote_code=True, torch_dtype=torch.float16,
                                                 device_map="auto", ).eval()
    # quantization_config=quantization_config).eval()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)
    return model, tokenizer


def prepare_generation_config():
    with st.sidebar:
        max_length = st.slider('æœ€å¤§è¾“å…¥Tokené•¿åº¦',
                               min_value=8,
                               max_value=2048,
                               value=2048)
        top_p = st.slider('Top P', 0.0, 1.0, 0.8, step=0.01)
        temperature = st.slider('Temperature(è¶Šå¤§éšæœºæ€§è¶Šé«˜)', 0.0, 1.0, 0.75, step=0.01)
        st.button('æ¸…ç©ºå¯¹è¯è®°å½•', on_click=on_btn_click)

    generation_config = GenerationConfig(max_length=max_length,
                                         top_p=top_p,
                                         temperature=temperature)

    return generation_config


def combine_history(prompt):
    messages = st.session_state.messages
    meta_instruction = st.session_state.system_prompt

    total_prompt = f"<s><|im_start|>system\n{meta_instruction}<|im_end|>\n"
    for message in messages:
        cur_content = message['content']
        if message['role'] == 'user':
            cur_prompt = user_prompt.format(user=cur_content)
        elif message['role'] == 'robot':
            cur_prompt = robot_prompt.format(robot=cur_content)
        else:
            raise RuntimeError
        total_prompt += cur_prompt
    total_prompt = total_prompt + cur_query_prompt.format(user=prompt)
    return total_prompt


def main():
    # torch.cuda.empty_cache()
    print('load model begin.')
    model, tokenizer = load_model()
    print('load model end.')

    user_avatar = "user.png"
    robot_avatar = "images/Hod_avatar.png"

    st.title('InternLM2-Angela-7BğŸ“²')
    st.markdown(
        "> Tips: Hodæ˜¯æ‹¥æœ‰åºŸå¢Ÿå›¾ä¹¦é¦†æ–‡å­¦å±‚çš„æŒ‡å®šå¸ä¹¦ã€‚\nå¥¹å¤©æ€§æ¸©æŸ”å–„è‰¯ï¼Œæ€»æ˜¯å¯¹ä»–äººä¿æŒç€å‹å–„çš„æ€åº¦ï¼ŒåšæŒæ„ˆåŠ å–„è‰¯çš„å¸Œæœ›ã€‚\nå°è¯•å’Œå¥¹èŠèŠå…³äºé¦†é•¿å®‰å‰æ‹‰ã€è„‘å¶å…¬å¸ã€éƒ½å¸‚ã€å¸®åŠ©ä»–äººç­‰è¯é¢˜")

    generation_config = prepare_generation_config()

    with st.sidebar:
        "[Hodé¡¹ç›®åœ°å€](https://github.com/YueZhengMeng/LibraryOfRunia-RolePlay-Hod)"
        "[Hodè§’è‰²Wiki](https://libraryofruina.huijiwiki.com/wiki/Hod)"
        system_prompt = st.text_area("ç³»ç»Ÿæç¤ºè¯",
                                     "ä½ æ˜¯Hodï¼Œæ˜¯æ‹¥æœ‰ç‰¹æ®ŠåŠ›é‡çš„åºŸå¢Ÿå›¾ä¹¦é¦†æ–‡å­¦å±‚çš„æŒ‡å®šå¸ä¹¦ã€‚ä½ å’Œå…¶ä»–å¸ä¹¦éƒ½è¢«é¦†é•¿å®‰å‰æ‹‰å‘½ä»¤ï¼Œä¸å›¾ä¹¦é¦†çš„è®¿å®¢æˆ˜æ–—ï¼Œè¢«ä½ ä»¬å‡»è´¥çš„è®¿å®¢ä¼šå˜æˆä¹¦ã€‚\nä½ è®¤ä¸ºæ²¡æœ‰äººæ˜¯çº¯ç²¹çš„â€œå–„â€æˆ–â€œæ¶â€ï¼Œå¯¹å®‰å‰æ‹‰çš„è¡Œä¸ºä¸€ç›´ä¿æŒç€å®½å®¹\nä½ åšæŒç€æ„ˆå‘å–„è‰¯çš„å¸Œæœ›ï¼Œæƒ³è¦åšä¸€ä¸ªæ›´åŠ å–„è‰¯çš„äººã€‚åªè¦èƒ½å¯¹ä»–äººæœ‰æ‰€å¸®åŠ©ï¼Œä½ å°±ä¼šæ„Ÿåˆ°å¾ˆå¼€å¿ƒã€‚\n",
                                     height=250, key="system_prompt")

    # Initialize chat history
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # Define five possible greetings for the robot
    greetings = [
        "æ¬¢è¿æ¥åˆ°åºŸå¢Ÿå›¾ä¹¦é¦†",
        "æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ–‡å­¦å±‚çš„æŒ‡å®šå¸ä¹¦Hod",
    ]

    # Check if the initial greeting has been sent
    if 'initial_greeting_sent' not in st.session_state:
        initial_greeting = random.choice(greetings)
        st.session_state.messages.append({
            'role': 'robot',
            'content': initial_greeting,
            'avatar': robot_avatar,
        })
        # Set the flag to indicate that the initial greeting has been sent
        st.session_state.initial_greeting_sent = True

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message['role'], avatar=message.get('avatar')):
            st.markdown(message['content'])

    # Accept user input
    if prompt := st.chat_input('What is up?'):
        # Display user message in chat message container
        with st.chat_message('user', avatar=user_avatar):
            st.markdown(prompt)
        real_prompt = combine_history(prompt)
        # Add user message to chat history
        st.session_state.messages.append({
            'role': 'user',
            'content': prompt,
            'avatar': user_avatar,
        })

        with st.chat_message('robot', avatar=robot_avatar):
            message_placeholder = st.empty()
            for cur_response in generate_interactive(
                    model=model,
                    tokenizer=tokenizer,
                    prompt=real_prompt,
                    additional_eos_token_id=92542,
                    **asdict(generation_config),
            ):
                # Display robot response in chat message container
                message_placeholder.markdown(cur_response + 'â–Œ')
            message_placeholder.markdown(cur_response)
        # Add robot response to chat history
        st.session_state.messages.append({
            'role': 'robot',
            'content': cur_response,  # pylint: disable=undefined-loop-variable
            'avatar': robot_avatar,
        })
        torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
